"""
Handlers para cada nodo del grafo de conversaciÃ³n principal.
"""
from typing import Dict, Any
from langchain_core.messages import AIMessage
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from src.graphs.troubleshooting import create_troubleshooting_graph
from src.config.settings import settings


def detect_intents(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Nodo para verificar las intenciones detectadas

    Args:
        state: Estado actual de la conversaciÃ³n

    Returns:
        Estado actualizado
    """
    print(f"ğŸ§  Intenciones detectadas: {state['intents']}")
    return state


def handle_general_inquiry(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Nodo para manejar consultas generales (nivel 1)

    Args:
        state: Estado actual de la conversaciÃ³n

    Returns:
        Estado con la respuesta generada
    """
    messages = state["messages"]
    intents = state["intents"]
    business_info = state["business_info"]

    # Crear respuesta basada en intenciones detectadas
    response = ""

    # Procesar cada tipo de intenciÃ³n
    if "saludo" in intents:
        response += f"Â¡Hola! Soy el asistente virtual de Taborra Alarmas. Â¿En quÃ© puedo ayudarte hoy?\n\n"

    if "despedida" in intents:
        response += "Â¡Gracias por contactarnos! Que tengas un excelente dÃ­a.\n\n"

    if "direccion" in intents:
        response += f"ğŸ“ Nuestra direcciÃ³n es: {business_info.get('direccion', 'No disponible')}.\n\n"

    if "horario" in intents:
        response += f"ğŸ•’ Nuestro horario de atenciÃ³n es: {business_info.get('horario', 'No disponible')}.\n\n"

    if "email" in intents:
        response += f"ğŸ“§ Nuestro email de contacto es: {business_info.get('email', 'No disponible')}.\n\n"

    if "telefono" in intents:
        response += f"ğŸ“ Nuestro telÃ©fono de contacto es: {business_info.get('telefono', 'No disponible')}.\n\n"

    if "whatsapp" in intents:
        response += f"ğŸ“± Nuestro WhatsApp es: {business_info.get('whatsapp', 'No disponible')}.\n\n"

    if "whatsapp_servicio_tecnico" in intents:
        response += f"ğŸ”§ Nuestro WhatsApp para servicio tÃ©cnico es: {business_info.get('whatsapp_servicio_tecnico', 'No disponible')}.\n\n"

    if "whatsapp_ventas" in intents:
        response += f"ğŸ’¼ Nuestro WhatsApp para ventas es: {business_info.get('whatsapp_ventas', 'No disponible')}.\n\n"

    if "whatsapp_administracion" in intents:
        response += f"ğŸ“Š Nuestro WhatsApp para administraciÃ³n es: {business_info.get('whatsapp_administracion', 'No disponible')}.\n\n"

    if "whatsapp_cobranza" in intents:
        response += f"ğŸ’° Nuestro WhatsApp para cobranza es: {business_info.get('whatsapp_cobranza', 'No disponible')}.\n\n"

    if "security" in intents:
        response += f"ğŸ” Nuestro nÃºmero de Security 24 es: {business_info.get('telefono_security', 'No disponible')}.\n\n"

    if "control_alarma" in intents:
        response += "âš ï¸ Lo siento, no puedo controlar la alarma por seguridad. Si necesitas ayuda con tu alarma, por favor contacta a nuestro servicio tÃ©cnico.\n\n"

    # Si hay intenciÃ³n de estado_alarma o escaneo_camaras pero el usuario no tiene nivel 3, informar
    if ("estado_alarma" in intents or "escaneo_camaras" in intents) and state["user_level"] < 3:
        response += "âš ï¸ Para acceder a informaciÃ³n sobre el estado de alarmas y cÃ¡maras, necesitas un nivel de acceso superior. Por favor, contacta a nuestro servicio tÃ©cnico.\n\n"

    # Si no se detectÃ³ ninguna intenciÃ³n especÃ­fica para informaciÃ³n general
    if not any(i in intents for i in ["saludo", "despedida", "direccion", "horario", "email", "telefono",
                                      "whatsapp", "whatsapp_servicio_tecnico", "whatsapp_ventas",
                                      "whatsapp_administracion", "whatsapp_cobranza", "security", "control_alarma"]):
        response = "No he podido entender tu consulta. Â¿Puedes especificar quÃ© informaciÃ³n necesitas sobre nuestros servicios?"

    # AÃ±adir respuesta al historial
    messages.append(AIMessage(content=response))

    return {**state, "messages": messages}


def handle_alarm_status(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Nodo para manejar consultas de estado de alarma (nivel 3)

    Args:
        state: Estado actual de la conversaciÃ³n

    Returns:
        Estado con la respuesta generada
    """
    messages = state["messages"]

    # SimulaciÃ³n:
    alarm_status = {
        "ParticiÃ³n 1": "activada",
        "ParticiÃ³n 2": "desactivada"
    }

    # Formatear respuesta
    response = "ğŸ“Š *Estado actual de la alarma:*\n\n"
    for partition, status in alarm_status.items():
        response += f"â€¢ {partition}: {status.upper()}\n"

    # AÃ±adir respuesta al historial
    messages.append(AIMessage(content=response))

    return {**state, "messages": messages}


def handle_camera_scan(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Nodo para manejar escaneo de cÃ¡maras (nivel 3)

    Args:
        state: Estado actual de la conversaciÃ³n

    Returns:
        Estado con la respuesta generada
    """
    messages = state["messages"]

    # SimulaciÃ³n:
    camera_data = [
        {"id": "camera.entrada_principal",
            "name": "Entrada Principal", "state": "Grabando"},
        {"id": "camera.patio_trasero", "name": "Patio Trasero", "state": "Grabando"},
        {"id": "camera.cocina", "name": "Cocina", "state": "Inactiva"}
    ]

    # Formatear respuesta
    response = "ğŸ“· *Estado de las cÃ¡maras:*\n\n"
    for camera in camera_data:
        response += f"â€¢ {camera['name']}: {camera['state']}\n"

    response += "\nEnviando imÃ¡genes de las cÃ¡maras activas..."

    # AÃ±adir respuesta al historial
    messages.append(AIMessage(content=response))

    return {**state, "messages": messages}


def start_troubleshooting(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Nodo para iniciar el flujo de resoluciÃ³n de problemas

    Args:
        state: Estado actual de la conversaciÃ³n

    Returns:
        Estado con el flujo de troubleshooting iniciado
    """
    # Preparar estado para el flujo de troubleshooting
    troubleshooting_state = {
        "messages": state["messages"],
        "current_step": 0,
        "keyboard_type": None,
        "problem_type": None,
        "solutions_shown": [],
        "rating": None,
        "business_info": state["business_info"]
    }

    return {
        **state,
        "troubleshooting_active": True,
        "troubleshooting_state": troubleshooting_state
    }


def process_troubleshooting(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Nodo para procesar el flujo de resoluciÃ³n de problemas

    Args:
        state: Estado actual de la conversaciÃ³n

    Returns:
        Estado con resultados del proceso de troubleshooting
    """
    # Obtener grafo de troubleshooting
    troubleshooting_graph = create_troubleshooting_graph()

    # Procesar estado con el grafo
    result = troubleshooting_graph.invoke(state["troubleshooting_state"])

    # Verificar si hemos terminado
    if result["current_step"] == 0 and len(result["messages"]) > 1:
        return {
            **state,
            "messages": result["messages"],
            "troubleshooting_active": False,
            "troubleshooting_state": None
        }

    # Continuar el flujo
    return {
        **state,
        "messages": result["messages"],
        "troubleshooting_state": result
    }


def handle_llm_response(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Nodo para generar respuestas usando LLM para casos no cubiertos por otros nodos

    Args:
        state: Estado actual de la conversaciÃ³n

    Returns:
        Estado con la respuesta generada por el modelo
    """
    messages = state["messages"]
    user_level = state["user_level"]
    intents = state["intents"]

    # Crear un template para el prompt
    prompt_template = """
    Eres el asistente virtual de Taborra Alarmas, una empresa de seguridad y alarmas.
    
    Nivel de acceso del usuario: {user_level} (1=general, 2=tÃ©cnico, 3=avanzado)
    
    Intenciones detectadas: {intents}
    
    Responde de manera amigable y profesional. Usa emojis ocasionalmente.
    
    Mensaje del usuario: {user_message}
    
    Tu respuesta:
    """

    # Crear el prompt
    prompt = ChatPromptTemplate.from_template(prompt_template)

    # Crear LLM
    llm = ChatOpenAI(
        model=settings.MODEL_NAME,
        temperature=0.7,
        openai_api_key=settings.OPENAI_API_KEY
    )

    # Crear la cadena
    chain = prompt | llm

    # Obtener Ãºltimo mensaje del usuario
    user_message = messages[-1].content if messages and hasattr(
        messages[-1], "content") else ""

    # Ejecutar la cadena
    response = chain.invoke({
        "user_level": user_level,
        "intents": ", ".join(intents),
        "user_message": user_message
    })

    # AÃ±adir respuesta al historial
    messages.append(AIMessage(content=response.content))

    return {**state, "messages": messages}


def finalize_response(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Nodo final que marca el tÃ©rmino del procesamiento

    Args:
        state: Estado actual de la conversaciÃ³n

    Returns:
        Estado sin modificaciones
    """
    return state
